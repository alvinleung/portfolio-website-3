---
title: Kitchen Experiment
description: Beyond touch interaction in the kitchen
tags: Human Computer Interaction, Research Through Design
type: case-study

colorAccent: rgb(249,255,241)
colorLight: rgb(249,255,241)
colorDark: rgb(28, 30, 38)
colorDarkest: rgb(28, 30, 38)
colorScheme: dark

nextProject: tedxsfu

previewVideo: https://www.sfu.ca/~kkl64/videos/kitchen-experiment/kitchen-experiment-cover-4.mp4

scope: 8 weeks project
weight: 10
---

<LayoutFull>
  <FullImage
    src="/project-assets/kitchen-experiment/intro.jpg"
    width={1792}
    height={926}
  />
</LayoutFull>

-- Modern home cooks rely heavily on digital recipes. Yet, amidst the sizzle and sweat, dirty hands makes navigating touch devices uncomfortable. This prototype-driven capstone project propose a multimodal interface for recipe navigation.

<Team
  teammates={[
    {
      name: "Collaborator",
      position: "Ethan Ma",
    },
    {
      name: "Supervisor",
      position: "Dr. Carman Neustaedter",
    },
  ]}
/>

<LayoutFull extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/assembly-top-down-1.mp4"
    canScrub={false}
  />
</LayoutFull>

<ColorShifter background="#F9FFF1" color="#444" />

## A Sticky Situation

As home cooks ourselves, we know the pain of navigating digital recipes. Slimy raw meat, dusty flour, and a million other things in the kitchen make you think twice before touching your devices. Because of this, we see the opportunity to _challenge the dominant touch paradigm_ of modern mobile devices.

## Touchless cooking companion

The solution features an air-gesture and voice-query-enabled cooking companion, allowing users to navigate recipes while cooking without ever touching their devices.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-play-pause-2.mp4"
    canScrub={false}
  >
    <Caption
      label="1"
      text="Users can play and pause a video by holding their hands in front of the device."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-scrub-square.mp4"
    canScrub={false}
  >
    <Caption
      label="2"
      text="As the user long-holds, they enter scrubbing mode. They can lean their hands to one side to scrub the video, similar to using arrow keys."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

<LayoutMainContent>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/voice-demo-wide-2.mp4"
    canScrub={false}
  >
    <Caption
      label="3"
      text="Users can enable voice queries when their hands are occupied with other tasks."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

<LayoutMainContent grid="2/3">
  <Image
    src="/project-assets/kitchen-experiment/voice-action.jpg"
    width={585 * 1.5}
    height={693 * 1.5}
  >
    <Caption
      label="4"
      text="Follow-up actions, such as setting a timer and pinning a card, can be accessed using voice commands right after a response."
      overlay={false}
    />
  </Image>
  <Image
    src="/project-assets/kitchen-experiment/voice-unit.jpg"
    width={585 * 1.5}
    height={693 * 1.5}
  >
    <Caption
      label="5"
      text="The measurements mentioned in the response will be highlighted beneath for ease of access. Users can toggle between units by tapping on the pills."
      topPadding
      overlay={false}
    />
  </Image>
</LayoutMainContent>

## Literature review and secondary research

The investigation began by surveying the domains of spatial computing and domestic computing. We have found intriguing exploration such as _[on-skin gesture control](https://www.tandfonline.com/doi/full/10.1080/14606925.2022.2058444)_, but that relies on custom hardware. On the consumer market, recipe apps typically focus on pre-cooking phases like recipe management and discovery. In during-cooking phase, voice enabled solutions are typically very primitive and clunky to navigate.

## Insights from ethnographic field study

To gain a more robust understanding of digital recipes navigation during cooking, we conducted contextual inquiries with 5 home cooks of varying skill levels. While users are generally able to complete the tasks through workarounds, the operations were less than desirable.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/elbow-interaction_6.mp4"
    width={1920}
    height={1080}
    canScrub={false}
  >
    <Caption
      label="4"
      title="Elbow Interaction"
      text="Elbow interaction forces users into an awkward position, often leading them to accidentally move the devices into an upright position."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/	knuckle-interaction_4.mp4"
    canScrub={false}
  >
    <Caption
      label="4"
      title="Knuckle Interaction"
      text="While affording higher precision, the digital device is prone to contamination due to its proximity to dirty fingers."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>
In addition, through affinity diagramming, we also surfaced the following major insights:

<List>
  <ListItem label="1">
    _Pictures speak a thousand word_. Multiple participants emphasized the
    importance of having visual references in the instructions. Videos or
    images, unlike text, allow learners to match their cooking intuitively. This
    is especially useful for novice cooks or those unfamiliar with the recipe.
  </ListItem>
  <ListItem label="2">
    _Cooking and recipe lookup introduces a high cognitive load_, as it takes
    away the user’s attention from performing their actual cooking tasks. Our
    design intervention needs to be as lightweight as possible.
  </ListItem>
  <ListItem label="3">
    _Participants move around the kitchen_, meaning they don’t necessarily have
    a line of sight with the digital device.
  </ListItem>
</List>

With more user context, we chose to adopt a multi-modal approach to redesign the experience. We identified the following key areas for further investigation:

<List>
  <ListItem label="1">Video as primary recipe medium</ListItem>
  <ListItem label="2">
    Camera feed-based air gestures to substitute touch interaction
  </ListItem>
  <ListItem label="3">
    Voice query to enable omni-directional interaction
  </ListItem>
</List>

## Experimentation with design engineering

The depth sensor on iOS devices enables the software to know how far away a gesture is performed. This capability is crucial for reliably preventing unwanted triggers of the gesture system.

To achieve that, I picked up Swift and developed a technique to process depth sensor data, which included capturing and smoothing user movements. This resulted in a platform that facilitated rapid iteration on different gesture approaches.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/sensor-feed-processing.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

## Air gesture design principles

In early experiments, we explored the idea of creating an air gesture pointer system. Similar to a desktop pointer, the user will control a vitual pointer by gestures. However, the idea was quickly abandoned as we found that using a pointer system is a rather intricate task that adds extra cognitive overhead.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-gesture-problem.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

Based on our experiments and secondary research, we used the following principles when designing the gesture language.

<List>
  <ListItem label="1">
    _Use natural spatial mapping_. Appeal to the user's existing mental modal
    whenever we could. Take the example for video scrubbing: people associate
    left with backward and right with forward.
  </ListItem>
  <ListItem label="2 ">
    _Continuous feedback_ to create a fluid feeling. Gestures needs to be
    interruptible, user can change their mind as they will. By doing so, it
    makes the user [feel like the interface is always responding to them
    (7:00)](https://developer.apple.com/videos/play/wwdc2018/803/), which is
    good for usability.
  </ListItem>
  <ListItem label="3">
    _Design for imprecision_ to reduce the cognitive load of operating the
    gestural interface. In our user studies, user perceived the larger interface
    elements to be easier to use. This make sense as the larger scale encourage
    them to take bigger and less precise gesture to control the interface.
    Consider the larger click area presents on Google Map's car ui.
  </ListItem>
</List>

To constrain our exploration, we have decided to focus our effort on defining the air-gesture scrubbing interaction first as this is the part that has the most complexity.

## Experiment: Drag to scrub

That lead us to experimenting with air gesture based “_drag to scrub_” interaction.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/58dc8648-a6a8-4976-93f5-9c18d216448d.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

While the scrubbing system was intuitive to understand, it suffer a few drawbacks that makes it particularly tricky to move forward with the direction.

<List>
  <ListItem label="1">
    The enter and exit motion may accidentally trigger scrubbing action. And
    this is difficault to address with software gesture rejection without
    sacrificing too much responsiveness.
  </ListItem>
  <ListItem label="2">
    This interaction also introduces a very awkward in-between state of jumping
    back as the user run out of the scrubbing area.
  </ListItem>
</List>

## Experiment: Dial to scrub

This iteration was utilize a circular dialling motion as video scrubbing. It solved the in-between state issue by making the scrubbing movement an infinite loop. The gesture was also intentionally referencing the knob-turning motion to appeal to the user's existing mental model.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

However it suffer from ergonomic issues. Users find it physically fatiguing to use
this gesture for a prolonged period of time because of the fine motor movement.

## Experiment: Lean to scrub

Then we were stuck, but then we reminded a user behaviour we observed in of our study a participant dash around a youtube video instruction just by holding down the arrow keys. This got us the idea of using “leaning” to capture user’s intent of video scrubbing.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

This was chosen to be included in the final solution because:

<List>
  <ListItem label="1">
    Physically not taxing to perform as the user just need to hold down a
    spatial position.
  </ListItem>
  <ListItem label="2">
    Provide continuous feedback with the subtle following motion that
    acknowledge user's input
  </ListItem>
  <ListItem label="3">
    With the additional hold state, it seemlessly incorporate the play, pause
    state into the design.
  </ListItem>
</List>

## Reflection—Anything flies in prototype-land

As a research-through-design project, our goal wasn’t to create a pristine high fidelity mockups but to gain insights from each step of prototyping. One particularly wonderful moment came when we essentially hacked together a ChatGPT voice interaction client and open a YouTube tab next to it. Despite its rough form, this prototype provided us with invaluable behavioral insights into how our potential solution could function.
