---
title: Kitchen Experiment
description: When touch is out of reach
tags: Human Computer Interaction, Research Through Design
type: case-study

colorAccent: rgb(249,255,241)
colorLight: rgb(235, 250, 214)
colorDark: rgb(23,58,50)
colorDarkest: rgb(23,58,50)
colorScheme: dark

nextProject: tedxsfu

previewVideo: https://www.sfu.ca/~kkl64/videos/kitchen-experiment/kitchen-experiment-cover-4.mp4

scope: 8 weeks project
weight: 10
---

<LayoutFull>
  <FullImage
    src="/project-assets/kitchen-experiment/intro-scroll2.jpg"
    width={1920}
    height={1682}
  />
</LayoutFull>

-- Amidst the sizzle and sweat, grime-coated hands render the use of touch devices not only inconvenient but distinctly uncomfortable. This prototype-driven project propose a multimodal interface for recipe navigation in the kitchen.

<Team
  teammates={[
    {
      name: "Collaborator",
      position: "Ethan Ma",
    },
    {
      name: "Supervisor",
      position: "Dr. Carman Neustaedter",
    },
  ]}
/>

<LayoutFull extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/assembly-top-down-1.mp4"
    canScrub={false}
  />
</LayoutFull>

<ColorShifter background="#F9FFF1" color="#444" />

## A sticky situation

Slimy raw meat, dusty flour, and a million other things in the kitchen make you think twice before touching your devices. As fellow home cooks, we know the frustration intimately.

Here, we saw the opportunity to challenge the dominant touch paradigm of modern mobile devices—say goodbye to your buttons, scroll views, and scrubbers.

## A touchless cooking companion

Our reimagined cooking experience integrates air-gesture and voice-query capabilities, allowing the user to focus on creating delicious meals, mess-free. No taps, flicks, or swipes—just seamless, intuitive interaction.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-play-pause-2.mp4"
    canScrub={false}
  >
    <Caption
      label="1"
      text="Users can play or pause a video by holding their hand in front of the device."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-scrub-square.mp4"
    canScrub={false}

>

    <Caption
      label="2"
      text="Long-hold to enter scrubbing mode: akin to using arrow keys, users can scrub through the video by leaning their hand to a side."
      overlay={false}
      topPadding
    />

  </Video>
</LayoutMainContent>

<LayoutMainContent>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/voice-demo-wide-2.mp4"
    canScrub={false}
  >
    <Caption
      label="3"
      text="Voice queries enable users to handle cooking tasks and look up recipes hands-free."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

<LayoutMainContent grid="2/3">
  <Image
    src="/project-assets/kitchen-experiment/voice-unit-2.jpg"
    width={585 * 1.5}
    height={693 * 1.5}
  >
    <Caption
      label="First"
      text='A common question during recipe lookup is "how much". Measurement is provided when voice answers mentioned an ingredient.'
      topPadding
      overlay={false}
    />
    <Caption
      label="Second"
      text="Follow-up actions allow users to start a timer or pin a response with just their voice."
      overlay={false}
      topPadding
    />
  </Image>
  <Image
    src="/project-assets/kitchen-experiment/voice-action-wide-light-3.jpg"
    width={2384 / 2}
    height={1386 / 2}
  />
</LayoutMainContent>

## Literature review and secondary research

Our investigation started with spatial and domestic computing domains in HCI literature. We found promising ideas like [on-skin gesture control](https://www.tandfonline.com/doi/full/10.1080/14606925.2022.2058444) but it requires custom hardware. While recipe apps on the market shine in pre-cooking—managing and discovering recipes, they often neglect the during-cooking phase. Additionally, voice-enabled solutions tend to be rudimentary and challenging to navigate.

## Ethnography study (definitely not a hoax to get them to cook for us)

To gain a more robust understanding of digital recipes navigation during cooking, we conducted contextual inquiries with 5 home cooks of varying skill levels.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://stream.mux.com/InB32NeYUsHpbZnHipYZF4ivmvxz01SHTNlvKGruyEis/capped-1080p.mp4"
    width={800}
    height={800}
    canScrub={false}
  >
    <Caption label="" text="" overlay={false} topPadding />
  </Video>
</LayoutMainContent>

During the sessions, while users managed to complete tasks with workarounds, the experience was suboptimal:

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/elbow-interaction_6.mp4"
    width={1920}
    height={1080}
    canScrub={false}
  >
    <Caption
      label="6"
      title="Observation: elbow interaction"
      text="Elbow interaction forces users into an awkward position, often leading them to accidentally move the devices into an upright position."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/	knuckle-interaction_4.mp4"
    canScrub={false}
  >
    <Caption
      label="7"
      title="Observation: knuckle interaction"
      text="While affording higher precision, the digital device is prone to contamination due to its proximity to dirty fingers."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

Through affinity diagramming, we also uncovered these key insights:

<List>
  <ListItem label="1">
    _Visual aids matter:_ Participants stressed the value of visual instructions. Videos and images help home cooks match their actions intuitively.

  </ListItem>
  <ListItem label="2">
    _Keep design interventions minimal:_ distractions from recipe lookups can be mentally taxing. Lightweight solutions help users stay focused on cooking.

  </ListItem>
  <ListItem label="3">
    _Participants move around the kitchen:_ Users move around the kitchen, often without direct sightlines to digital devices, emphasizing the need for accessible, hands-free solutions.

  </ListItem>
</List>

These insights guided our exploration. After generating ideas, we pursued a multi-modal approach, focusing on these primary investigation areas:

<List>
  <ListItem label="1">Video as primary recipe medium</ListItem>
  <ListItem label="2">
    Camera feed-based air gestures to substitute touch interaction
  </ListItem>
  <ListItem label="3">
    Voice query to enable omni-directional interaction
  </ListItem>
</List>

## Design + engineering

Leveraging iOS TrueDepth Camera, we can detect gesture distances and prevent unwanted triggers.

To design with this sensor, I developed a technique to process depth sensor data, capturing and smoothing user movements using Swift. The resulted platform allowed quick testing of various gesture approaches.

<LayoutFull extraMargin>
  <Video
    src="https://stream.mux.com/GuBJZ00FoNWKiocOl02KVEQpNB014cbSCou02SI9M65wUQ8/capped-1080p.mp4"
    width={1792}
    height={1052}
  />
</LayoutFull>

## Air gesture design principles

In early experiments, we explored the idea of creating an air gesture pointer system. But we soon find out that it was a bad idea: as _manipulating a pointer on screen is a rather intricate task that adds cognitive overhead_. They are better off interacting with features directly.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-gesture-problem.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

We then proceed with the following gesture principles with our research.

<List>
  <ListItem label="1">
    _Use natural spatial mapping_: when gesturing left, the visuals should move
    left too. It would be confusing if they moved in the opposite direction.
  </ListItem>
  <ListItem label="2">
    _Appeal to the user's existing mental model whenever possible_: for video
    scrubbing, people associate left with backward and right with forward.
  </ListItem>
  <ListItem label="3">
    _Continuous, instant visual feedback is crucial_. Gestures must always respond to the user's input, [even if they change their mind mid-action](https://developer.apple.com/videos/play/wwdc2018/803/). This builds trust in the interface, allowing users to interact without hesitation and encouraging exploration of new interactions.

  </ListItem>
  <ListItem label="4"> 
    _Design for imprecision to lighten cognitive load_. In our studies, users find larger interface elements easier because they encourage broader, less precise gestures.

  </ListItem>
</List>

To refocus our exploration, we have decided to tackle the more complex air-gesture scrubbing first.

### Attempt 1: wave to scrub

That lead us to experimenting with air gesture based wave to scrub” interaction.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/58dc8648-a6a8-4976-93f5-9c18d216448d.mp4?_a=DATAdtAAZAA0"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>

While intuitive, the scrubbing system suffers a few drawbacks that makes it particularly tricky to move forward.

<List>
  <ListItem label="1">
    Entering and exiting the motion may accidentally trigger scrubbing, which is
    difficult to fix without sacrificing responsiveness.
  </ListItem>
  <ListItem label="2">
    This interaction also introduces a very awkward in-between-swipe state of
    users have to lift their hands off the detection zone to restart swiping.
  </ListItem>
</List>

### Attempt 2: Dial to scrub

This iteration used a circular scrubbing motion, mimicking knob-turning. The circular motion solves the in-between-swipe motion.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>

Yet, aside from unresolved rejection on enter/exit motion, it also suffer from ergonomic issues. Users find it physically fatiguing to use
this gesture for a prolonged period of time because of the fine motor control involved.

### 3rd times the charm: Lean to scrub

We were initially stuck, but then we recalled an interesting behavior from our study: a participant navigated quickly through a YouTube video by holding down the arrow keys. This observation inspired us to explore the concept of using “leaning” to capture a user’s intent when scrubbing through a video.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://stream.mux.com/yVziBc4eEEknQ6AcEoHeL027wynPFUGtBpCtCeGwFQWI/capped-1080p.mp4"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>

This was ultimately chosen to be included in the final solution because:

<List>
  <ListItem label="1">
    It is easy to perform for the user, as they only need to maintain a static
    position. This requires less motor effort compared to the constant movement
    needed for other solution.
  </ListItem>
  <ListItem label="2">
    With the additional hold state, it seemlessly incorporate the play, pause
    state into the design.
  </ListItem>
  <ListItem label="3">
    It still provides continuous feedback with subtle hints that acknowledge the
    user’s input.
  </ListItem>
</List>

<Quote who="Dr. Carman Neustaedter" title="Supervisor">
  {
    "The interaction—of pausing it, moving forward, backwards— is actually going really, really well. I am very impressed with that, very nicely done."
  }
</Quote>

## Takeaway: in praise of the barely working prototype

We kept delaying testing sessions, feeling we weren’t “ready.” It wasn’t until we set a hard deadline of “no prototype, no dinner” that we forced ourselves to test our design. Surprisingly, the prototypes we thought weren’t ready turned out to be incredibly valuable for learning.

<LayoutMainContent grid="2/3" extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/chat-gpt-demo.jpg"
    width={3024 / 4}
    height={4032 / 4}
  >
    <Caption
      label="9"
      text="One wonderful moment was when we hacked together a ChatGPT voice interaction client and opened a YouTube tab next to it. Despite its rough form, this prototype gave us invaluable insights into how our solution could function."
      overlay={false}
      topPadding
    />
  </Image>
</LayoutMainContent>

## Takeaway: idea happens when you make things

During the process of prototyping the air gesture, we explored various techniques for detecting hand gestures. As we experimented with different methods, we discovered new ways to detect user input and potential possibilities for interaction emerged.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://stream.mux.com/KNxHbSyrt4AjSQ00nhLFD017CEU5tC021KPuhJzZ1yhadQ/capped-1080p.mp4"
    width={1080}
    height={1920}
    canScrub={true}
  >

<Caption
      label="10"
      text="Because we developed the dynamic bounding box technique, we were able to to explore a circular air-gestures."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

## Takeaway: testing just need to make sense

Formative qualitative testing and feedback don't have to be overly complex or formal to be valuable; they just need to make sense. The ultimate goal is to expand your thinking in the problem space.

In our case, we frequently tested our idea with friends and family. By observing how they interacted with our prototype and interpreting their reactions, we were able to catch a lot of issues in the design. When approach with caution, guerrilla testing on the go is a powerful tool.

## Appendix: a fascinated dad

My dad tried to scrub video with his head—slightly less violant than pointing a knife at the prototype(yes, someone did try the interaction with a knife)

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://stream.mux.com/jHx00zHu02rMPr6q7G02dmvP7k02EjetS2IP4hq02nJkQcIk/capped-1080p.mp4"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>
