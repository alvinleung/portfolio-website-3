---
title: Kitchen Experiment
description: Beyond touch interaction in the kitchen
tags: Human Computer Interaction, Research Through Design
type: case-study

colorAccent: rgb(249,255,241)
colorLight: rgb(235, 250, 214)
colorDark: rgb(23,58,50)
colorDarkest: rgb(23,58,50)
colorScheme: dark

nextProject: tedxsfu

previewVideo: https://www.sfu.ca/~kkl64/videos/kitchen-experiment/kitchen-experiment-cover-4.mp4

scope: 8 weeks project
weight: 10
---

<LayoutFull>
  <FullImage
    src="/project-assets/kitchen-experiment/intro-scroll2.jpg"
    width={1920}
    height={1682}
  />
</LayoutFull>

-- Modern home cooks rely heavily on digital recipes. Yet, amidst the sizzle and sweat, dirty hands makes navigating touch devices uncomfortable. This prototype-driven capstone project propose a multimodal interface for recipe navigation.

<Team
  teammates={[
    {
      name: "Collaborator",
      position: "Ethan Ma",
    },
    {
      name: "Supervisor",
      position: "Dr. Carman Neustaedter",
    },
  ]}
/>

<LayoutFull extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/assembly-top-down-1.mp4"
    canScrub={false}
  />
</LayoutFull>

<ColorShifter background="#F9FFF1" color="#444" />

## A Sticky Situation

As home cooks ourselves, we know the pain of navigating digital recipes intimately. Slimy raw meat, dusty flour, and a million other things in the kitchen make you think twice before touching your devices.

Because of this, we see the opportunity to challenge the dominant touch paradigm of modern mobile devices. So no more tap, no more flick, no more swipes. No more buttons, no more scroll views, no more scrubber. Is it even possible?

## A touchless cooking companion

Our reimagined experience features an air-gesture and voice-query-enabled cooking companion, allowing users to navigate recipes while cooking without ever touching their devices.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-play-pause-2.mp4"
    canScrub={false}
  >
    <Caption
      label="1"
      text="Users can play and pause a video by holding their hands in front of the device."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-scrub-square.mp4"
    canScrub={false}
  >
    <Caption
      label="2"
      text="As the user long-holds, they enter scrubbing mode. By tilting their hands to one side, they can scrub through the video, akin to using arrow keys."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

<LayoutMainContent>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/voice-demo-wide-2.mp4"
    canScrub={false}
  >
    <Caption
      label="3"
      text="Users can enable voice queries when their hands are occupied with other tasks."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

<LayoutMainContent grid="2/3">
  <Image
    src="/project-assets/kitchen-experiment/voice-unit-2.jpg"
    width={585 * 1.5}
    height={693 * 1.5}
  >
    <Caption
      label="First"
      text="The measurements mentioned in the response will be highlighted beneath for ease of access. Users can toggle between units by tapping on the pills."
      topPadding
      overlay={false}
    />
    <Caption
      label="Second"
      text="Each answer has a follow-up action such as setting a timer and pinning an answer. They can be accessed by saying the voice commands right after a response."
      overlay={false}
      topPadding
    />
  </Image>
  <Image
    src="/project-assets/kitchen-experiment/voice-action-wide-light-3.jpg"
    width={2384 / 2}
    height={1386 / 2}
  />
</LayoutMainContent>

## Literature review and secondary research

The investigation began by surveying the domains of spatial computing and domestic computing. We have found intriguing exploration such as [on-skin gesture control](https://www.tandfonline.com/doi/full/10.1080/14606925.2022.2058444), but that relies on custom hardware. On the consumer market, recipe apps typically focus on pre-cooking phases like recipe management and discovery. In during-cooking phase, voice enabled solutions are typically very primitive and clunky to navigate.

## Insights from ethnographic study

To gain a more robust understanding of digital recipes navigation during cooking, we conducted contextual inquiries with 5 home cooks of varying skill levels. While users are generally able to complete the tasks through workarounds, the operations were less than desirable.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/elbow-interaction_6.mp4"
    width={1920}
    height={1080}
    canScrub={false}
  >
    <Caption
      label="6"
      title="Elbow Interaction"
      text="Elbow interaction forces users into an awkward position, often leading them to accidentally move the devices into an upright position."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/	knuckle-interaction_4.mp4"
    canScrub={false}
  >
    <Caption
      label="7"
      title="Knuckle Interaction"
      text="While affording higher precision, the digital device is prone to contamination due to its proximity to dirty fingers."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>
In addition, through affinity diagramming, we also surfaced the following major insights:

<List>
  <ListItem label="1">
    _Pictures speak a thousand word_. Multiple participants emphasized the
    importance of having visual references in the instructions. Videos or
    images, unlike text, allow learners to match their cooking intuitively. This
    is especially useful for novice cooks or those unfamiliar with the recipe.
  </ListItem>
  <ListItem label="2">
    _Cooking and recipe lookup introduces a high cognitive load_, as it takes
    away the user’s attention from performing their actual cooking tasks. Our
    design intervention needs to be as lightweight as possible.
  </ListItem>
  <ListItem label="3">
    _Participants move around the kitchen_, meaning they don’t necessarily have
    a line of sight with the digital device.
  </ListItem>
</List>

The insight served as a backdrop where the exploration commences. After ideating a few alternative methods to interact with devices, we chose to adopt a multi-modal approach to redesign the experience. We identified the following key areas for further investigation:

<List>
  <ListItem label="1">Video as primary recipe medium</ListItem>
  <ListItem label="2">
    Camera feed-based air gestures to substitute touch interaction
  </ListItem>
  <ListItem label="3">
    Voice query to enable omni-directional interaction
  </ListItem>
</List>

## Experimentation with design engineering

To experiment with air gesture, I used Swift to developed a technique to process depth sensor data, which included capturing and smoothing user movements. This resulted in a platform that facilitated rapid iteration on different gesture approaches.

<LayoutFull extraMargin>
  <Video
    src="https://stream.mux.com/8C3UF9xK3O86ZCLz5jtQhSZy1MhYxJlncCXfnJX01RI00/capped-1080p.mp4"
    width={1792}
    height={1052}
  >
    <Caption
      label="8"
      wideSpacing
      text="The following motion graphic demostrates the signal processing pipeline. Depth sensor on iOS devices enables the software to know how far away a gesture is performed. This capability is crucial for reliably preventing unwanted triggers."
      overlay={true}
      topPadding
    />
  </Video>
</LayoutFull>

## Air gesture design principles

In early experiments, we explored the idea of creating an air gesture pointer system. Similar to a desktop pointer, the user will control a vitual pointer by gestures. However, the idea was quickly abandoned as we found that using a pointer system is a rather intricate task that adds extra cognitive overhead.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-gesture-problem.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

Based on our experiments and secondary research, we then proceed with the following principles when designing the gesture language.

<List>
  <ListItem label="1">
    _Use natural spatial mapping_. When gesturing to the left side, the visual
    should move left as well. It will be very jarring if the visual were to move
    in an opposite way.
  </ListItem>
  <ListItem label="2">
    _Appeal to the user's existing mental modal_ whenever we could. Take the
    example for video scrubbing: people associate left with backward and right
    with forward.
  </ListItem>
  <ListItem label="3">
    _Continuous, instant visual feedback_. Gestures need to respond to user's
    input all the time, if they change their mind, gestures need to respond to
    that interruption. This establish trust towards the interface, the user
    would not have to [think before performing an action (WWDC,
    7:00)](https://developer.apple.com/videos/play/wwdc2018/803/), which
    encourages the user to explore the novel interaction.
  </ListItem>
  <ListItem label="4">
    _Design for imprecision_ to reduce the cognitive load of operating the
    gestural interface. In our user studies, user perceived the larger interface
    elements to be easier to use. This make sense as the larger scale nudges the
    users to take bigger and less precise gesture to control the interface.
  </ListItem>
</List>

To constrain our exploration, we have decided to focus our effort on defining the air-gesture scrubbing interaction first as this is the part that has the most complexity.

## Attempt 1: Drag to scrub

That lead us to experimenting with air gesture based “_drag to scrub_” interaction.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/58dc8648-a6a8-4976-93f5-9c18d216448d.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

While the scrubbing system was intuitive to understand, it suffer a few drawbacks that makes it particularly tricky to move forward.

<List>
  <ListItem label="1">
    The enter and exit motion may accidentally trigger scrubbing action. And
    this is difficault to address with software gesture rejection without
    sacrificing too much responsiveness.
  </ListItem>
  <ListItem label="2">
    This interaction also introduces a very awkward in-between-swipe state of
    jumping back as the user run out of the scrubbing area.
  </ListItem>
</List>

## Attempt 2: Dial to scrub

This iteration was utilize a circular dialling motion as video scrubbing. It solved the in-between-swipe state issue by making the scrubbing movement an infinite loop. The gesture was also intentionally referencing the knob-turning motion to appeal to the user's existing mental model.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

However it suffer from ergonomic issues. Users find it physically fatiguing to use
this gesture for a prolonged period of time because of the fine motor control required.

## Third time the charm: Lean to scrub

We were stuck, but then we remembered a user behaviour observed in of our study. The participant dash around a youtube video instruction just by holding down the arrow keys. This got us the idea of using “leaning” to capture user’s intent of video scrubbing.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

This was ultimately chosen to be included in the final solution because:

<List>
  <ListItem label="1">
    Physically not taxing to perform as the user just need to hold down a
    spatial position.
  </ListItem>
  <ListItem label="2">
    Provide continuous feedback with the subtle following motion that
    acknowledge user's input
  </ListItem>
  <ListItem label="3">
    With the additional hold state, it seemlessly incorporate the play, pause
    state into the design.
  </ListItem>
</List>

It was particularly rewarding when we have the approval of our supervisor:

## Reflection—Anything flies in prototype-land

As a research-through-design project, our goal wasn’t to create a pristine high fidelity mockups but to gain insights from each step of prototyping. One particularly wonderful moment came when we essentially hacked together a ChatGPT voice interaction client and open a YouTube tab next to it. Despite its rough form, this prototype provided us with invaluable behavioral insights into how our potential solution could function. This really drive home the idea that prototypes are good as long as they provide us glimpes of of the final resault.
