---
title: Kitchen Experiment
description: Beyond touch interaction in the kitchen
tags: Human Computer Interaction, Research Through Design
type: case-study

colorAccent: rgb(235,237,232)
colorLight: rgb(235,237,232)
colorDark: rgb(28, 30, 38)
colorDarkest: rgb(28, 30, 38)
colorScheme: dark

nextProject: tedxsfu

previewVideo: https://www.sfu.ca/~kkl64/videos/kitchen-experiment/kitchen-experiment-cover-3.mp4

scope: 8 weeks project
weight: 10
---

<LayoutFull>
  <FullImage
    src="/project-assets/kitchen-experiment/intro.jpg"
    width={1792}
    height={926}
  />
</LayoutFull>

-- Modern home cooks rely heavily on digital recipes. Yet, amidst the sizzle and sweat, dirty hands makes navigating touch devices undesirable. This prototype-driven capstone project propose a multimodal interface for recipe navigation.

<Team
  teammates={[
    {
      name: "Collaborator",
      position: "Ethan Ma",
    },
    {
      name: "Supervisor",
      position: "Dr. Carman Neustaedter",
    },
  ]}
/>

<ColorShifter background="rgb(235,237,232)" color="#444" />

<LayoutFull extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/assembly-top-down-1.mp4"
    canScrub={false}
  />
</LayoutFull>

## A Sticky Situation

The pain of navigating such digital recipes—video or textural—with dirty hands is all too familiar. Slimy raw meat, dusty flour, there are million of things in the kitchen that makes you think twice before touching your devices. Because of this, we see the oppoutinity to _challenge the dominant touch paradigm_ of modern mobile devices.

## Putting everything together

Video demo of the solution here

## Literature review and secondary research

The project began by surveying the domain of spatial computing and domestic computing. We have found intriguing exploration such as _[on-skin gesture control](https://www.tandfonline.com/doi/full/10.1080/14606925.2022.2058444)_, but that relies on custom hardware. On the consumer market, recipe apps typically focus on pre-cooking phases like recipe management and discovery. In during-cooking phase, voice enabled solutions are typically very primitive and clunky to navigate.

## Insights from ethnographic field study

To gain a more robust understanding of digital recipes navigation during cooking, we conducted contextual inquiries with 5 home cooks of varying skill levels. Through affinity diragramm we surfaced the following major insights:

<List>
  <ListItem label="1">
    _Pictures speak a thousand word_. Mutiple participants emphasised the
    importance of having visual reference in the instruction. Videos or images,
    unlike text, let learners match their cooking intuitively. This is
    especially useful for novice cooks or those unfamiliar with the recipe.
  </ListItem>
  <ListItem label="2">
    _Cooking and recipe lookup introduces high cognitive load_ as it take away
    user’s attention to perform their actual cooking task. Our design
    intervention needs to be as lightweight as possible.
  </ListItem>
  <ListItem label="3">
    _Participants move around the kitchen_, meaning they don’t necessarily have
    line of sight with the digital device — especially tablet.
  </ListItem>
  <ListItem label="4">
    _Undersiable workarounds_—While users are generally able to complete the
    tasks through workarounds, the operations were less than desirable.
  </ListItem>
</List>

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/elbow-interaction_6.mp4"
    width={1920}
    height={1080}
    canScrub={false}
  >
    <Caption
      label="4"
      title="Elbow Interaction"
      text="Elbow interaction force user into an awkward position, leading them moving the devices accidntally on up-right positions."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/	knuckle-interaction_4.mp4"
    canScrub={false}
  >
    <Caption
      label="4"
      title="Knuckle Interaction"
      text="While affording higher precision, the digital device is prone to contamination because of its proximity with the dirty fingers."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

With more user context, we decided to take a multi-modal approach and established the following lines of investigation:

<List>
  <ListItem label="1">Video as primary recipe medium</ListItem>
  <ListItem label="2">
    Camera feed as user input to enable air gestures.
  </ListItem>
  <ListItem label="3">
    Voice query as a omni-directional mode of interaction to retrieve recipe
    information. This is useful when user is not facing the device
  </ListItem>
</List>

## Experimentation with design engineering

Originally designed for FaceID, the depth sensor front facing camera provide a reliable way to extract air-gesture's distance. I picked up swift and developed an unique technique to process depth sensor feed which captures and smoothes out the user’s movement. This provided a platform where we can try and iterate on different gesture approaches rapidly.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/sensor-feed-processing.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

## Air gesture design principles

In early experiments, we played around with the idea of creating a air gesture pointer system—like on a desktop. But the idea was quickly abandoned as we found using pointer system is a rather intricate task that creates extra cognitive overhead.

<LayoutMainContent grid="1/3" extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-cursor-system.png"
    width={655}
    height={671}
  />
</LayoutMainContent>

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-gesture-problem.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

As a result of our own experiment and secondary research, we consolidated the following principles for designing an intuitive gesture language.

<List>
  <ListItem label="1">
    _Use natural spatial mapping_ to match the user mental modal of the real
    world.
  </ListItem>
  <ListItem label="2 ">
    _Continuous feedback_ to create an intuitive, fluid feeling. (apple
    designing fluid interfaces) Its also means that the gesture is
    interruptible, user can change direction, exit as they will.
  </ListItem>
  <ListItem label="3">
    _Design for imprecision_ to reduce the cognitive load of operating the
    gestural interface. In our user studies, user perceived the larger interface
    elements to be easier to use. This make sense as the larger scale encourage
    them to take bigger and less precise gesture to control the interface.
    Consider the larger click area presents on Google Map's car ui.
  </ListItem>
</List>

### Experiment 1: Drag to scrub

That lead us to experimenting with air gesture based “_drag to scrub_” interaction.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/58dc8648-a6a8-4976-93f5-9c18d216448d.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

While the scrubbing system was intuitive to understand, it suffer a few drawbacks that makes it particularly tricky to move forward with the direction.

<List>
  <ListItem label="1">
    The enter and exit motion may accidentally trigger scrubbing action. And
    this is difficault to address with software gesture rejection without
    sacrificing too much responsiveness.
  </ListItem>
  <ListItem label="2">
    This interaction also introduces a very awkward in-between state of jumping
    back as the user run out of the scrubbing area.
  </ListItem>
</List>

### Experiment 2: Dial to scrub

This iteration was utilize a circular dialling motion as video scrubbing. It solved the in-between state issue by making the scrubbing movement an infinite loop. The gesture was also intentionally referencing the knob-turning motion to appeal to the user's existing mental model.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

However it suffer from ergonomic issues. Users find it physically fatiguing to use
this gesture for a prolonged period of time because of the fine motor movement.

### Experiment 3: Lean to scrub

Then we were stuck, but then we reminded a user behaviour we observed in of our study a participant dash around a youtube video instruction just by holding down the arrow keys. This got us the idea of using “leaning” to capture user’s intent of video scrubbing.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

This was chosen to be included in the final solution because:

<List>
  <ListItem label="1">
    Physically not taxing to perform as the user just need to hold down a
    spatial position.
  </ListItem>
  <ListItem label="2">
    Provide continuous feedback with the subtle following motion that
    acknowledge user's input
  </ListItem>
  <ListItem label="3">
    With the additional hold state, it seemlessly incorporate the play, pause
    state into the design.
  </ListItem>
</List>
