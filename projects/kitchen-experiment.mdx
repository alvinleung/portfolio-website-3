---
title: Kitchen Experiment
description: Beyond touch interaction in the kitchen
tags: Human Computer Interaction, Research Through Design
type: case-study

colorAccent: rgb(235,237,232)
colorLight: rgb(235,237,232)
colorDark: rgb(28, 30, 38)
colorDarkest: rgb(28, 30, 38)
colorScheme: dark

nextProject: idagio

previewVideo: https://www.sfu.ca/~kkl64/videos/kitchen-experiment/kitchen-experiment-cover-3.mp4

scope: 8 weeks project
weight: 10
---

<LayoutFull>
  <FullImage
    src="/project-assets/kitchen-experiment/intro.jpg"
    width={1792}
    height={926}
  />
</LayoutFull>

-- Modern home cooks rely heavily on digital recipes. Yet, amidst the sizzle and sweat, dirty hands makes navigating touch devices undesirable. This prototype-driven capstone project propose a multimodal interface for recipe navigation.

<Team
  teammates={[
    {
      name: "Collaborator",
      position: "Ethan Ma",
    },
    {
      name: "Supervisor",
      position: "Dr. Carman Neustaedter",
    },
  ]}
/>

<ColorShifter background="rgb(235,237,232)" color="#444" />

<LayoutFull extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/assembly-top-down-1.mp4"
    canScrub={false}
  />
</LayoutFull>

## A Sticky Situation

The pain of navigating such digital recipes—video or textural—with dirty hands is all too familiar. Slimy raw meat, dusty flour, there are million of things in the kitchen that makes you think twice before touching your devices. Because of this, we see the oppoutinity to _challenge the dominant touch paradigm_ of modern mobile devices.

The project began by surveying the domain of spatial computing and domestic computing. In academia(HCI), we have found intriguing exploration such as on-skin gesture control, but that relies on custom hardware. At app store, recipe apps typically focus on pre-cooking phases like recipe management and discovery. Voice enabled solutions are typically very primitive and clunky to operate.

Simply put, there is an unmet need for a consumer-oriented, touch-free cooking experience.

## Insights from Contextual Inquiries

To probe deeper, we conducted contextual inquiries with five home cooks of varying skill levels to observe how they navigate digital recipes. While users are generally able to complete the tasks through workarounds, the operations were less the desirable.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/elbow-interaction_6.mp4"
    width={1920}
    height={1080}
    canScrub={false}
  >
    <Caption
      label="4"
      title="Elbow Interaction"
      text="Elbow interaction force user into an awkward position, leading them moving the devices accidntally on up-right positions."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/	knuckle-interaction_4.mp4"
    canScrub={false}
  >
    <Caption
      label="4"
      title="Knuckle Interaction"
      text="While affording higher precision, the digital device is prone to contamination because of its proximity with the dirty fingers."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

Aside from that, we also notice the following:

<List>
  <ListItem label="1">
    _Pictures speak a thousand word_. Participants emphasised the importance of
    having visual reference in the instruction. Videos or images, unlike text,
    let learners match their cooking intuitively. This is especially useful for
    novice cooks or those unfamiliar with the recipe.
  </ListItem>
  <ListItem label="2">
    _Cooking and recipe lookup introduces high cognitive load_ as it take away
    user’s attention to perform their actual cooking task. Our design
    intervention needs to be as lightweight as possible.
  </ListItem>
  <ListItem label="3">
    _Participants move around the kitchen_, meaning they don’t necessarily have
    line of sight with the digital device — especially tablet.
  </ListItem>
</List>

With more user context, we decided to take a multi-modal approach and established the following lines of investigation:

<List>
  <ListItem label="1">Video as primary recipe medium</ListItem>
  <ListItem label="2">
    Camera feed as user input to enable air gestures
  </ListItem>
  <ListItem label="3">
    Voice query as a omni-directional mode of interaction to retrieve recipe
    information
  </ListItem>
</List>

## Iterating with design engineering

I needed to tinker around with the code to observe what can be done with the technology—this is the essence of the "research through design" methodology.

In order to experiment with hand gestures, I developed a unique technique to process depth sensor feed which captures and smooth out the user’s movement. This provided a platform where we can try and iterate on different gesture approaches rapidly.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/sensor-feed-processing.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

## Air gesture design principles

In early experiments, we played around with the idea of creating a air gesture pointer system—like on a desktop. But the idea was quickly abandoned as we found using pointer system is a rather intricate task that creates extra cognitive overhead.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-gesture-problem.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

As a result of our own experiment and secondary research, here are a few principle that we adhere to when designing an intuitive gesture language.

<List>
  <ListItem label="1">
    _Use natural spatial mapping_ to match the user mental modal of the real
    world.
  </ListItem>
  <ListItem label="2">
    _Continuous feedback_ to create an intuitive, fluid feeling. (apple
    designing fluid interfaces) Its also means that the gesture is
    interruptible, user can change direction, exit as they will.
  </ListItem>
  <ListItem label="3">
    _Design for imprecision_ to reduce the cognitive load of operating the
    gestural interface. In our user studies, user perceived the larger interface
    elements to be easier to use. This make sense as the larger scale encourage
    them to take bigger and less precise gesture to control the interface.
  </ListItem>
</List>

That lead us to experimenting with air gesture based “_drag to scrub_” interaction.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/58dc8648-a6a8-4976-93f5-9c18d216448d.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>

While the scrubbing system was easy to understand, it suffer a few drawbacks. It was particularly tricky to make “drag to scrub” a viable path
forward.

<List>
  <ListItem label="1">
    The enter and exit motion may accidentally trigger scrubbing action. And
    this is difficault to address with software gesture rejection without
    sacrificing too much responsiveness.
  </ListItem>
  <ListItem label="2">
    This interaction also introduces a very awkward in-between state of jumping
    back as the user run out of the scrubbing area.
  </ListItem>
</List>

So we tried again. After a lot of sketching(and air gesturing), we arrived at this second promising line of investigation. This iteration was utilize a circular dialling motion as video scrubbing.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1920}
    height={1080}
    canScrub={true}
  />
</LayoutMainContent>
However it suffer from ergonomic issues. Users find it physically fatiguing to use
this gesture for a prolonged period of time.

## A deceptively simple solution

Then we were stuck, but then we reminded a user behaviour we observed in of our study. A participant dash around a youtube video instruction. This got us the idea of using “leaning” to capture user’s intent of video scrubbing.
