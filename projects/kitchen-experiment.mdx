---
title: Kitchen Experiment
description: Beyond touch interaction in the kitchen
tags: Human Computer Interaction, Research Through Design
type: case-study

colorAccent: rgb(249,255,241)
colorLight: rgb(235, 250, 214)
colorDark: rgb(23,58,50)
colorDarkest: rgb(23,58,50)
colorScheme: dark

nextProject: tedxsfu

previewVideo: https://www.sfu.ca/~kkl64/videos/kitchen-experiment/kitchen-experiment-cover-4.mp4

scope: 8 weeks project
weight: 10
---

<LayoutFull>
  <FullImage
    src="/project-assets/kitchen-experiment/intro-scroll2.jpg"
    width={1920}
    height={1682}
  />
</LayoutFull>

-- Modern home cooks rely heavily on digital recipes. Yet, amidst the sizzle and sweat, dirty hands makes navigating touch devices uncomfortable. This prototype-driven capstone project propose a multimodal interface for recipe navigation.

<Team
  teammates={[
    {
      name: "Collaborator",
      position: "Ethan Ma",
    },
    {
      name: "Supervisor",
      position: "Dr. Carman Neustaedter",
    },
  ]}
/>

<LayoutFull extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/assembly-top-down-1.mp4"
    canScrub={false}
  />
</LayoutFull>

<ColorShifter background="#F9FFF1" color="#444" />

## A sticky situation

As home cooks ourselves, we know the pain of navigating digital recipes intimately. Slimy raw meat, dusty flour, and a million other things in the kitchen make you think twice before touching your devices.

Because of this, we see the opportunity to challenge the dominant touch paradigm of modern mobile devices. So no more tap, no more flick, no more swipes. No more buttons, no more scroll views, no more scrubber. Is it even possible?

## A touchless cooking companion

Our reimagined experience features an air-gesture and voice-query-enabled cooking companion, allowing users to navigate recipes while cooking without ever touching their devices.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-play-pause-2.mp4"
    canScrub={false}
  >
    <Caption
      label="1"
      text="Users can play or pause a video by holding their hand in front of the device."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/gesture-scrub-square.mp4"
    canScrub={false}

>

    <Caption
      label="2"
      text="As the user long-holds, they enter scrubbing mode. By tilting their hands to one side, they can scrub through the video, akin to using arrow keys."
      overlay={false}
      topPadding
    />

  </Video>
</LayoutMainContent>

<LayoutMainContent>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/voice-demo-wide-2.mp4"
    canScrub={false}
  >
    <Caption
      label="3"
      text="Users can enable voice queries when their hands are occupied with other tasks."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

<LayoutMainContent grid="2/3">
  <Image
    src="/project-assets/kitchen-experiment/voice-unit-2.jpg"
    width={585 * 1.5}
    height={693 * 1.5}
  >
    <Caption
      label="First"
      text="The measurements mentioned in the response will be highlighted beneath for ease of access. Users can toggle between units by tapping on the pills."
      topPadding
      overlay={false}
    />
    <Caption
      label="Second"
      text="Each answer has a follow-up action such as setting a timer and pinning an answer. They can be accessed by saying the voice commands right after a response."
      overlay={false}
      topPadding
    />
  </Image>
  <Image
    src="/project-assets/kitchen-experiment/voice-action-wide-light-3.jpg"
    width={2384 / 2}
    height={1386 / 2}
  />
</LayoutMainContent>

## Literature review and secondary research

The investigation began by surveying the domains of spatial computing and domestic computing. We have found intriguing exploration such as [on-skin gesture control](https://www.tandfonline.com/doi/full/10.1080/14606925.2022.2058444), but that relies on custom hardware. On the consumer market, recipe apps typically focus on pre-cooking phases like recipe management and discovery. In during-cooking phase, voice enabled solutions are typically very primitive and clunky to navigate.

## Insights from ethnographic study

To gain a more robust understanding of digital recipes navigation during cooking, we conducted contextual inquiries with 5 home cooks of varying skill levels. While users are generally able to complete the tasks through workarounds, the operations were less than desirable.

<LayoutMainContent grid="2/3" extraMargin>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/elbow-interaction_6.mp4"
    width={1920}
    height={1080}
    canScrub={false}
  >
    <Caption
      label="6"
      title="Elbow Interaction"
      text="Elbow interaction forces users into an awkward position, often leading them to accidentally move the devices into an upright position."
      overlay={false}
      topPadding
    />
  </Video>
  <Video
    src="https://www.sfu.ca/~kkl64/videos/kitchen-experiment/	knuckle-interaction_4.mp4"
    canScrub={false}
  >
    <Caption
      label="7"
      title="Knuckle Interaction"
      text="While affording higher precision, the digital device is prone to contamination due to its proximity to dirty fingers."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

In addition, through affinity diagramming, we also surfaced the following major insights:

<List>
  <ListItem label="1">
    _Pictures speak a thousand words_. Multiple participants emphasized the
    importance of having visual references in the instructions. Videos or
    images, unlike text, allow learners to match their cooking intuitively. This
    is especially useful for novice cooks or those unfamiliar with the recipe.
  </ListItem>
  <ListItem label="2">
    _Design intervention needs to be as lightweight as possible_ to keep users
    focused on cooking. Recipe lookups can be distracting and mentally taxing,
    pulling attention away from the actual cooking tasks.
  </ListItem>
  <ListItem label="3">
    _Participants move around the kitchen_, meaning they don’t necessarily have
    a line of sight with the digital device.
  </ListItem>
</List>

The insight served as a backdrop where the exploration commences. After ideating a few alternative methods to interact with devices, we chose to adopt a multi-modal approach to redesign the experience. We identified the following key areas for further investigation:

<List>
  <ListItem label="1">Video as primary recipe medium</ListItem>
  <ListItem label="2">
    Camera feed-based air gestures to substitute touch interaction
  </ListItem>
  <ListItem label="3">
    Voice query to enable omni-directional interaction
  </ListItem>
</List>

## Experimentation with design engineering

To experiment with air gesture, I used Swift to developed a technique to process depth sensor data, which included capturing and smoothing user movements. This resulted in a platform that facilitated rapid iteration on different gesture approaches.

<LayoutFull extraMargin>
  <Video
    src="https://stream.mux.com/8C3UF9xK3O86ZCLz5jtQhSZy1MhYxJlncCXfnJX01RI00/capped-1080p.mp4"
    width={1792}
    height={1052}
  >
    <Caption
      label="8"
      wideSpacing
      text="The following motion graphic demostrates the signal processing pipeline. Depth sensor on iOS devices enables the software to know how far away a gesture is performed. This capability is crucial for reliably preventing unwanted triggers."
      overlay={true}
      topPadding
    />
  </Video>
</LayoutFull>

## Air gesture design principles

In early experiments, we explored the idea of creating an air gesture pointer system. But we soon find out that it was a bad idea: as _manipulating a pointer on screen is a rather intricate task that adds cognitive overhead_.

<LayoutFull extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/air-gesture-problem.jpg"
    width={1792}
    height={1053}
  />
</LayoutFull>

We then proceed with the following gesture principles with our research.

<List>
  <ListItem label="1">
    _Use natural spatial mapping_. When gesturing to the left side, the visual
    should move left as well. It will be very jarring if the visual were to move
    in an opposite way.
  </ListItem>
  <ListItem label="2">
    _Appeal to the user's existing mental modal_ whenever we could. Take the
    example for video scrubbing: people associate left with backward and right
    with forward.
  </ListItem>
  <ListItem label="3">
    _Continuous, instant visual feedback_. Gestures need to respond to user's
    input all the time, if they change their mind, gestures need to respond to
    that interruption. This establish trust towards the interface, the user
    would not have to [think before performing an action (WWDC,
    7:00)](https://developer.apple.com/videos/play/wwdc2018/803/), which
    encourages the user to explore the novel interaction.
  </ListItem>
  <ListItem label="4">
    _Design for imprecision_ to reduce the cognitive load. In our user studies,
    user perceived the larger interface elements to be easier to use. This make
    sense as the larger scale nudges the users to take bigger and less precise
    gesture to control the interface.
  </ListItem>
</List>

To constrain our exploration, we have decided to focus our effort on defining the air-gesture scrubbing interaction first as this is the part that has the most complexity.

## Attempt 1: wave to scrub

That lead us to experimenting with air gesture based wave to scrub” interaction. It make sense if we think about translating the 2d haptic scroll experience to a 3d air-gesture space.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/58dc8648-a6a8-4976-93f5-9c18d216448d.mp4?_a=DATAdtAAZAA0"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>

While the scrubbing system was intuitive to understand, it suffer a few drawbacks that makes it particularly tricky to move forward.

<List>
  <ListItem label="1">
    Entering and exiting the motion may accidentally trigger scrubbing, which is
    difficult to fix without sacrificing responsiveness.
  </ListItem>
  <ListItem label="2">
    This interaction also introduces a very awkward in-between-swipe state of
    users have to lift their hands to restart swiping.
  </ListItem>
</List>

## Attempt 2: Dial to scrub

This iteration used a circular dialing motion for video scrubbing. It resolved the in-between-swipe issue by making the scrubbing an infinite loop. The gesture also referenced knob-turning to align with the user’s existing mental model.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://res.cloudinary.com/read-cv/video/upload/t_v_b/v1/1/profileItems/elxDbGufYVdRx2SBPOeM2V30vWr2/QocppSyo5cTqixN0OOaR/15a10979-b424-4fe3-806d-046f503a55d6.mp4?_a=DATAdtAAZAA0"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>

Yet, aside from unresolved rejection on enter/exit motion, it alsO suffer from ergonomic issues. Users find it physically fatiguing to use
this gesture for a prolonged period of time because of the fine motor control involved.

## Takeaway: 3rd times the charm: Lean to scrub

We were initially stuck, but then we recalled an interesting behavior from our study: a participant navigated quickly through a YouTube video by holding down the arrow keys. This observation inspired us to explore the concept of using “leaning” to capture a user’s intent when scrubbing through a video.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://stream.mux.com/yVziBc4eEEknQ6AcEoHeL027wynPFUGtBpCtCeGwFQWI/capped-1080p.mp4"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>

This was ultimately chosen to be included in the final solution because:

<List>
  <ListItem label="1">
    It is not physically taxing for the user, as they only need to maintain a
    static position. This requires less motor effort compared to the constant
    movement needed for other solution.
  </ListItem>
  <ListItem label="2">
    It still provides continuous feedback with subtle hints that acknowledge the
    user’s input.
  </ListItem>
  <ListItem label="3">
    With the additional hold state, it seemlessly incorporate the play, pause
    state into the design.
  </ListItem>
</List>

<Quote who="Dr. Carman Neustaedter" title="Supervisor">
  {
    "The interaction—of pausing it, moving forward, backwards— is actually going really, really well. I am very impressed with that, very nicely done."
  }
</Quote>

## Takeaway: in praise of the barely working prototype

We kept delaying testing sessions, feeling we weren’t “ready.” It wasn’t until we set a hard deadline of “no prototype, no dinner” that we forced ourselves to test our design. Surprisingly, the prototypes we thought weren’t ready turned out to be incredibly valuable for learning.

<LayoutMainContent grid="2/3" extraMargin>
  <Image
    src="/project-assets/kitchen-experiment/chat-gpt-demo.jpg"
    width={3024 / 4}
    height={4032 / 4}
  >
    <Caption
      label="9"
      text="One wonderful moment was when we hacked together a ChatGPT voice interaction client and opened a YouTube tab next to it. Despite its rough form, this prototype gave us invaluable insights into how our solution could function."
      overlay={false}
      topPadding
    />
  </Image>
</LayoutMainContent>

## Takeaway: idea happens when you make things

During the process of prototyping the air gesture, we explored various techniques for detecting hand gestures. As we experimented with different methods, we discovered new ways to detect user input and potential possibilities for interaction emerged.

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://stream.mux.com/KNxHbSyrt4AjSQ00nhLFD017CEU5tC021KPuhJzZ1yhadQ/capped-1080p.mp4"
    width={1080}
    height={1920}
    canScrub={true}
  >

<Caption
      label="10"
      text="Because we developed the dynamic bounding box technique, we were able to to explore a circular air-gestures."
      overlay={false}
      topPadding
    />
  </Video>
</LayoutMainContent>

## Takeaway: Testing just need to make sense

Formative qualitative testing and feedback don't have to be overly complex or rigorous to be valuable; they just need to make sense. The ultimate goal is to expand your thinking in the problem space.

In our case, we frequently tested our idea with friends and family. By observing how they interacted with our prototype and interpreting their reactions, we were able to catch a lot of issues in the design. Even though our testing was not overly elaborate, it still provided us with valuable insights that helped us to better understand our users' needs and preferences. In the end, this approach helped us to create a more effective and user-friendly product.

## Appendix: a fascinated dad

My dad tried to scrub video with his head—slighlty less violant than pointing a knife at the prototype(yes, someone did try the interaction with a knife)

<LayoutMainContent grid="1/3" extraMargin>
  <Video
    src="https://stream.mux.com/jHx00zHu02rMPr6q7G02dmvP7k02EjetS2IP4hq02nJkQcIk/capped-1080p.mp4"
    width={1080}
    height={1920}
    canScrub={true}
  />
</LayoutMainContent>
